version: "3.6"
services:
  # Master
  master-node:
    image: "binhvd/spark-cluster:fixed"
    hostname: master-node
    networks:
      - cluster-net
    ports:
      - "8080:8080"   # Spark Master UI
      - "8088:8088"   # YARN ResourceManager
      - "9870:9870"   # HDFS NameNode UI
      - "8888:8888"   # Jupyter Notebook
      - "18080:18080" # Spark History Server
      - "9000:9000"   # HDFS NameNode
      - "7077:7077"   # Spark Master Port
    volumes:
      - hdfs-master-index:/home/hadoop/data/nameNode
      - hdfs-master-checkpoint:/home/hadoop/data/namesecondary
      - hdfs-master-data:/home/hadoop/data/dataNode
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role==manager
  # Workers
  worker:
    image: "binhvd/spark-cluster:fixed"
    networks:
      - cluster-net
    volumes:
      - hdfs-worker-data:/home/hadoop/data/dataNode
    depends_on:
      - "master-node"
    deploy:
      mode: global
      placement:
        constraints:
          - node.role!=manager
volumes:
  hdfs-master-index:
    name: 'hdfs_master_index'
  hdfs-master-checkpoint:
    name: 'hdfs_master_checkpoint'
  hdfs-master-data:
    name: 'hdfs_master_data'
  hdfs-worker-data:
    name: 'hdfs_worker_data'
networks:
  cluster-net:
    driver: overlay
    driver_opts:
      com.docker.network.driver.mtu: 1280
